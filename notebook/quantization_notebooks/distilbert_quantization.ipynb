{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5Ne3Ael7nRxt",
        "outputId": "915af343-deee-4100-e739-bfb8c968af51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum quanto onnxruntime onnxruntime-tools onnxconverter_common -q"
      ],
      "metadata": {
        "id": "UTMxH8CCoQK1",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading finetuned model"
      ],
      "metadata": {
        "id": "v-ecSHd0OQt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import onnx\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification\n",
        "from pathlib import Path\n",
        "from transformers.onnx import FeaturesManager\n",
        "from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "from onnxconverter_common import float16"
      ],
      "metadata": {
        "id": "pE1oXQnu6sbA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_checkpoint = \"/content/drive/MyDrive/intent_classification/fine_tuned_distilled_bert\""
      ],
      "metadata": {
        "id": "RgBaRDZ6o_kf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(fine_tuned_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_checkpoint)"
      ],
      "metadata": {
        "id": "tEhrXn8Y2Z9v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_pipeline = pipeline(\"text-classification\", model=fine_tuned_model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "wlDfuPFIMvIR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_pipeline(\"Hey, you up to play some games today?\")"
      ],
      "metadata": {
        "id": "eGxZ6E1xM2dz",
        "outputId": "b9152cb2-b652-4b09-b39e-6ed0f8e18376",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'play games', 'score': 0.9896244406700134}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting model to .onnx format"
      ],
      "metadata": {
        "id": "KMPK8pcUOcRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load config\n",
        "feature = \"sequence-classification\"\n",
        "model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(fine_tuned_model, feature=feature)\n",
        "onnx_config = model_onnx_config(fine_tuned_model.config)\n",
        "\n",
        "# export\n",
        "onnx_inputs, onnx_outputs = transformers.onnx.export(\n",
        "        preprocessor=tokenizer,\n",
        "        model=fine_tuned_model,\n",
        "        config=onnx_config,\n",
        "        # output_fp16=True,\n",
        "        opset=13,\n",
        "        output=Path(\"/content/drive/MyDrive/intent_classification/onnx_model/trfs-model.onnx\")\n",
        ")"
      ],
      "metadata": {
        "id": "MCljSH0mCMk6",
        "outputId": "2daceb57-f221-472e-8a3d-f75ceac791a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py:231: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantizing .onnx model to int8"
      ],
      "metadata": {
        "id": "vI2MWn6uOicg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model_path = \"/content/drive/MyDrive/intent_classification/onnx_model/trfs-model.onnx\"\n",
        "quantized_model_path = \"/content/drive/MyDrive/intent_classification/quantint_model/quantint_trfs-model.onnx\""
      ],
      "metadata": {
        "id": "cPyGJni2LCyZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantize_dynamic(onnx_model_path,\n",
        "                  quantized_model_path,\n",
        "                  weight_type=QuantType.QInt8)\n",
        "\n",
        "# Save the model configuration\n",
        "fine_tuned_model.config.to_json_file(\"/content/drive/MyDrive/intent_classification/quantint_model/config.json\")"
      ],
      "metadata": {
        "id": "Rog0C0wDK7RH",
        "outputId": "431856f9-ab6b-4523-d86f-930dcc16b017",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferencing 8-bit quantized model\n"
      ],
      "metadata": {
        "id": "_Ub4XvbbOwCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantint_checkpoint = \"/content/drive/MyDrive/intent_classification/quantint_model\"\n",
        "tokenizer_checkpoint = \"/content/drive/MyDrive/intent_classification/fine_tuned_distilled_bert\""
      ],
      "metadata": {
        "id": "WTKU57PFLP7p"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantint_model = ORTModelForSequenceClassification.from_pretrained(quantint_checkpoint)"
      ],
      "metadata": {
        "id": "0O-AlUCTL3Tj",
        "outputId": "68e595de-1ca9-4002-c259-3527a483e5b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The ONNX file quantint_trfs-model.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantint_pipeline = pipeline(\"text-classification\", model=quantint_model, tokenizer=tokenizer_checkpoint)"
      ],
      "metadata": {
        "id": "0OjebZx0Lm-E"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantint_pipeline(\"send a mail to my manager\")"
      ],
      "metadata": {
        "id": "TUe3m6zSLw93",
        "outputId": "559b734d-314e-4acc-acc1-c5a366e35aa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'send email', 'score': 0.9977303147315979}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('ONNX full precision model size (MB):', os.path.getsize(onnx_model_path)/(1024*1024))\n",
        "print('ONNX quantized model size (MB):', os.path.getsize(quantized_model_path)/(1024*1024))"
      ],
      "metadata": {
        "id": "hGtsG7NTPNNy",
        "outputId": "f811d518-9740-4682-b320-9eaeded69e3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX full precision model size (MB): 255.6012372970581\n",
            "ONNX quantized model size (MB): 64.26772499084473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FP-16 Quantization"
      ],
      "metadata": {
        "id": "3iN6IHJRTnox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = onnx.load(onnx_model_path)\n",
        "model_fp16 = float16.convert_float_to_float16(model)\n",
        "onnx.save(model_fp16, \"/content/drive/MyDrive/intent_classification/quantfloat_model/quantfloat_trfs-model.onnx\")\n",
        "fine_tuned_model.config.to_json_file(\"/content/drive/MyDrive/intent_classification/quantfloat_model/config.json\")"
      ],
      "metadata": {
        "id": "gc4qg4QwTZvX",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp16_quantized_model = \"/content/drive/MyDrive/intent_classification/quantfloat_model/quantfloat_trfs-model.onnx\""
      ],
      "metadata": {
        "id": "b1TtTgo3VZE-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp16_quantized_checkpoint = \"/content/drive/MyDrive/intent_classification/quantfloat_model\""
      ],
      "metadata": {
        "id": "zUZ7QtgEWtsr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferencing fp16 quantized model"
      ],
      "metadata": {
        "id": "wnL--kH3Zi9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantfloat_model = ORTModelForSequenceClassification.from_pretrained(fp16_quantized_checkpoint)"
      ],
      "metadata": {
        "id": "5_0cqxofVjVJ",
        "outputId": "a5652f17-8b9e-450b-c976-02a0aa3889cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The ONNX file quantfloat_trfs-model.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantfloat_model = pipeline(\"text-classification\", model=quantfloat_model, tokenizer=tokenizer_checkpoint)"
      ],
      "metadata": {
        "id": "GLtaGapFWnk5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantfloat_model(\"send a mail to my manager\")"
      ],
      "metadata": {
        "id": "ESsA_9xqY5jw",
        "outputId": "4f6a4901-6141-4ec2-a1a4-76662053a9cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'send email', 'score': 0.998046875}]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('ONNX full precision model size (MB):', os.path.getsize(onnx_model_path)/(1024*1024))\n",
        "print('ONNX quantized model size (MB):', os.path.getsize(fp16_quantized_model)/(1024*1024))"
      ],
      "metadata": {
        "id": "U6cwbwNHY88L",
        "outputId": "27a9f00b-0db3-4ff1-d56f-964e0d6f2539",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX full precision model size (MB): 255.6012372970581\n",
            "ONNX quantized model size (MB): 127.90854835510254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking inference time b/w finetuned, quantized-float and quantized-int model"
      ],
      "metadata": {
        "id": "AMCt9ris3DoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "fine_tuned_pipeline(\"send a mail to my manager\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFCtBNXk2GZr",
        "outputId": "3865de66-508c-4e98-e7f9-cbcc39bad8e5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 82.4 ms, sys: 0 ns, total: 82.4 ms\n",
            "Wall time: 118 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'send email', 'score': 0.9980485439300537}]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "quantfloat_model(\"send a mail to my manager\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgCgItkh3Stl",
        "outputId": "0f0fa044-ce07-459c-dbe0-ba2ee20fca5b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 170 ms, sys: 64.2 ms, total: 234 ms\n",
            "Wall time: 252 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'send email', 'score': 0.998046875}]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "quantint_pipeline(\"send a mail to my manager\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D01B4vv-3UVH",
        "outputId": "28bb025f-9522-4cf1-acf4-468d794e23d2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 16 ms, sys: 319 Âµs, total: 16.4 ms\n",
            "Wall time: 43.6 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'send email', 'score': 0.9977303147315979}]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C2lDbA303WAY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}